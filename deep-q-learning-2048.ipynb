{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning with the game 2048\n",
    "\n",
    "This repository trains a q deep learning network from the game 2048 and plots a performance graph. The gamelogic is based on the implementation from Georg Wiese https://github.com/georgwiese/2048-rl . The deep q learning code is loosely based on the implementation from this repository https://github.com/keon/deep-q-learning, but was improved and adapted to include the game 2048.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### The game 2048\n",
    "\n",
    "2048 is a single-player sliding block puzzle game developed by Gabriele Cirulli in 2014. The game represents a 4 Ã— 4 grid where the value of each cell is a power of 2. An action can be any of the 4 movements: up, down, left right. When an action is performed, all cells move in the chosen direction. Any two adjacent cells with the same value (power of 2) along this direction merge to form one single cell with value equal to the sum of the two cells (i.e. the next power of 2). The objective of the game is to combine cells until reaching 2048.\n",
    "\n",
    "### Artificial Neural Network\n",
    "\n",
    "The deep network is a standard artificial neural network consisting of two fully connected hidden layers with 256 nodes each. As activation functions ReLu was used for all layers, which guarantees non vanishing gradients. The loss was computed using mse, as optimizer we used Adam. Choosing different configurations then mse and Adam didn't yield much different results.\n",
    "\n",
    "\n",
    "The hyperparameters can be set in the file parameters.py.\n",
    "\n",
    "The most successful run was achieved using the following configuration:\n",
    "\n",
    "gamma = 0.00001    # discount rate\n",
    "epsilon_decay = 0.9995\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "two hidden layers with 256 nodes each\n",
    "reward = score of the game, consisting of the highest tile achieved. E.g. if tile 256 was achieved, the score of this game was 256.\n",
    "\n",
    "\n",
    "\n",
    "To start training, execute all cells. To start plotting the graph, execute all cells of plot.ipynb, located in the root folder DQN-2048.\n",
    "\n",
    "The repo consists of two parts: the learning part and the full programmed game of 2048.\n",
    "The gamelogic of the game 2048 can be found in the folder gamelogic in file game.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the libraries and the gamelogic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "import time\n",
    "from shutil import copyfile\n",
    "import parameters\n",
    "import os\n",
    "\n",
    "from gamelogic.game import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, define the number of episodes to train the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 0\n",
      "episodes: 1\n",
      "episodes: 2\n",
      "episodes: 3\n",
      "episodes: 4\n",
      "episodes: 5\n",
      "episodes: 6\n",
      "episodes: 7\n",
      "episodes: 8\n",
      "episodes: 9\n",
      "episodes: 10\n",
      "episodes: 11\n",
      "episodes: 12\n",
      "episodes: 13\n",
      "episodes: 14\n",
      "episodes: 15\n",
      "episodes: 16\n",
      "episodes: 17\n",
      "episodes: 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f2f5cfb2aed0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_max_value_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\ReinforcementLearning\\DQN-2048\\gamelogic\\game.py\u001b[0m in \u001b[0;36mdo_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_random_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\ReinforcementLearning\\DQN-2048\\gamelogic\\game.py\u001b[0m in \u001b[0;36madd_random_tile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0mempty_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mempty_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mempty_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 100000\n",
    "\n",
    "path  = os.getcwd()\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = 16\n",
    "        self.action_size = 4 # (up, down, right, left)\n",
    "        self.memory = deque(maxlen=5000000)\n",
    "        self.gamma = parameters.gamma    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = parameters.epsilon_decay\n",
    "        self.learning_rate = parameters.learning_rate\n",
    "        self.model = self._build_model()\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.is_max_value_reward = parameters.is_max_value_reward\n",
    "        self.max_value_reward_threshold = parameters.max_value_reward_threshold\n",
    "        self.max_value_reward_amount = parameters.max_value_reward_amount\n",
    "        self.output_name = parameters.output_name\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='relu'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"algorithm tends to forget the previous experiences as it overwrites them with new experiences.\n",
    "        Therefore we re-train the model with previous experiences.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(game.available_actions())\n",
    "        #forward feeding\n",
    "        act_values = self.model.predict(state)\n",
    "        #sets q-values of not available actions to -100 so they are not chosen\n",
    "        if len(game.available_actions())< 4:\n",
    "          temp = game.available_actions()\n",
    "          for i in range(0, 4):\n",
    "            if i not in temp:\n",
    "              act_values[0][i] = -100\n",
    "        #returns action with highest q-value\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"trains the neural net with experiences from memory (minibatches)\"\"\"\n",
    "        #samples mimibatch from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        #for each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #if its final state set target to the reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                #set target according to formula\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            #gets all 4 predictions from current state\n",
    "            target_f = self.model.predict(state)\n",
    "            #takes the one action which was selected in batch\n",
    "            target_f[0][action] = target\n",
    "            #trains the model\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = Game()\n",
    "    agent = DQNAgent()\n",
    "    # agent.load(\"./save/file\")\n",
    "    done = False\n",
    "    batch_size = agent.batch_size\n",
    "    debug = False\n",
    "    save_maxvalues = True\n",
    "    output_list = []\n",
    "\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        game.new_game()\n",
    "        state = game.state()\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "        while not game.game_over():\n",
    "            action = agent.act(state)\n",
    "            reward = (game.do_action(action))**2\n",
    "            if(agent.is_max_value_reward):\n",
    "                reward = 0\n",
    "                temp = game.state()\n",
    "                temp_reshaped = np.reshape(temp, [1, agent.state_size])\n",
    "                temp_max_value = np.amax(temp_reshaped[0])\n",
    "                if temp_max_value > agent.max_value_reward_threshold:\n",
    "                    reward = agent.max_value_reward_amount\n",
    "            next_state = game.state()\n",
    "            actions_available = game.available_actions()\n",
    "            if len(actions_available) == 0: \n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "            next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if (debug): print(\"no action available\")\n",
    "                states = game.state()\n",
    "                states = np.reshape(state, [1, agent.state_size])\n",
    "                max_value = np.amax(states[0])\n",
    "                output_list.append([e, np.asscalar(max_value), np.asscalar(game.score()), agent.epsilon])\n",
    "                if(debug):print(\"max_value: \" + str(max_value))\n",
    "                break\n",
    "        print(\"episodes: \" + str(e))\n",
    "\n",
    "        #save copy of configuration and the episode_maxvalue_data\n",
    "        if save_maxvalues:\n",
    "            if e == 100:\n",
    "                src = path + \"/learn.py\"\n",
    "                dst = path + \"/data/\"+agent.output_name+\"config.py\"\n",
    "                copyfile(src, dst)\n",
    "                output_list.insert(0, \"gamma: \"+str(parameters.gamma)+\" | epsilon decay: \"+str(parameters.epsilon_decay)+\" | learning rate: \"+str(parameters.learning_rate)+\"\\n batch size: \"+str(parameters.batch_size)+\" | reward = maxVal: \"+str(parameters.is_max_value_reward)+\" | reward amount: \"+str(parameters.max_value_reward_amount)+\" | reward threshold: \"+str(parameters.max_value_reward_threshold))\n",
    "            if e % 100 == 0:\n",
    "                with open(path + \"/data/\"+agent.output_name+\"output.txt\", \"w\") as outfile:\n",
    "                    json.dump(output_list, outfile)\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        if e % 10000 == 0:\n",
    "            timenow = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            savepath = path + \"/data/agent\"+agent.output_name+timenow+\"_Epi\"+str(e)\n",
    "            agent.save(savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
